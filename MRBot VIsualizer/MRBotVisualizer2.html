<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Visualizer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1158797123946133"
     crossorigin="anonymous"></script>
    <script>
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    colors: {
                        primary: '#5D5CDE',
                    }
                }
            }
        }
    </script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7"></script>
</head>
<body class="bg-white dark:bg-gray-900 text-gray-800 dark:text-gray-200 transition-colors duration-200">
    <div class="container mx-auto px-4 py-8">
        <h1 class="text-3xl font-bold text-center mb-8">Transformer Architecture Visualizer</h1>
        
        <div class="mb-8">
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                <!-- Transformer Model Overview -->
                <div class="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-md">
                    <h2 class="text-xl font-semibold mb-4">Transformer Model Overview</h2>
                    <div id="transformer-diagram" class="w-full h-96 bg-gray-100 dark:bg-gray-700 rounded-lg p-4 overflow-auto">
                        <!-- Transformer diagram will be rendered here -->
                    </div>
                </div>
                
                <!-- Attention Visualization -->
                <div class="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-md">
                    <h2 class="text-xl font-semibold mb-4">Attention Visualization</h2>
                    <div class="mb-4">
                        <label for="input-text" class="block text-sm font-medium mb-2">Enter Text:</label>
                        <textarea id="input-text" rows="3" class="w-full px-3 py-2 text-base border rounded-md dark:bg-gray-700 dark:border-gray-600" 
                                  placeholder="Enter a sentence to visualize attention">The transformer model processes input through self-attention layers.</textarea>
                    </div>
                    <button id="visualize-btn" class="px-4 py-2 bg-primary text-white rounded-md hover:bg-opacity-90 transition-colors">
                        Visualize Attention
                    </button>
                    <div id="attention-visualization" class="mt-4 h-64 bg-gray-100 dark:bg-gray-700 rounded-lg p-4 overflow-auto">
                        <!-- Attention visualization will be rendered here -->
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Interactive Transformer Demo -->
        <div class="bg-white dark:bg-gray-800 p-6 rounded-lg shadow-md">
            <h2 class="text-xl font-semibold mb-4">Interactive Transformer Demo</h2>
            <div class="mb-4">
                <label for="demo-input" class="block text-sm font-medium mb-2">Enter Input Text:</label>
                <input type="text" id="demo-input" class="w-full px-3 py-2 text-base border rounded-md dark:bg-gray-700 dark:border-gray-600" 
                       placeholder="Enter text to process" value="Hello world">
            </div>
            <div class="mb-4">
                <button id="process-btn" class="px-4 py-2 bg-primary text-white rounded-md hover:bg-opacity-90 transition-colors">
                    Process Input
                </button>
            </div>
            <div id="processing-steps" class="space-y-4">
                <div class="p-4 bg-gray-100 dark:bg-gray-700 rounded-lg">
                    <h3 class="font-medium mb-2">1. Tokenization</h3>
                    <div id="tokenization-output" class="text-sm"></div>
                </div>
                <div class="p-4 bg-gray-100 dark:bg-gray-700 rounded-lg">
                    <h3 class="font-medium mb-2">2. Embedding + Positional Encoding</h3>
                    <div id="embedding-output" class="text-sm"></div>
                </div>
                <div class="p-4 bg-gray-100 dark:bg-gray-700 rounded-lg">
                    <h3 class="font-medium mb-2">3. Self-Attention</h3>
                    <div id="attention-output" class="text-sm"></div>
                </div>
                <div class="p-4 bg-gray-100 dark:bg-gray-700 rounded-lg">
                    <h3 class="font-medium mb-2">4. Feed-Forward Network</h3>
                    <div id="ffn-output" class="text-sm"></div>
                </div>
                <div class="p-4 bg-gray-100 dark:bg-gray-700 rounded-lg">
                    <h3 class="font-medium mb-2">5. Final Output</h3>
                    <div id="final-output" class="text-sm"></div>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Check dark mode preference
        if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            document.documentElement.classList.add('dark');
        }
        
        // Listen for dark mode changes
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', event => {
            if (event.matches) {
                document.documentElement.classList.add('dark');
            } else {
                document.documentElement.classList.remove('dark');
            }
        });

        // Transformer diagram
        function drawTransformerDiagram() {
            const width = document.getElementById('transformer-diagram').clientWidth;
            const height = document.getElementById('transformer-diagram').clientHeight;
            
            const svg = d3.select('#transformer-diagram')
                .append('svg')
                .attr('width', width)
                .attr('height', height);
            
            // Define architecture components
            const components = [
                { id: 'input', label: 'Input Embeddings', x: width/2, y: 50, width: 120, height: 40 },
                { id: 'pos', label: 'Positional Encoding', x: width/2, y: 110, width: 120, height: 40 },
                { id: 'attention', label: 'Multi-Head Attention', x: width/2, y: 190, width: 140, height: 40 },
                { id: 'norm1', label: 'Add & Norm', x: width/2, y: 250, width: 100, height: 30 },
                { id: 'ffn', label: 'Feed Forward Network', x: width/2, y: 310, width: 140, height: 40 },
                { id: 'norm2', label: 'Add & Norm', x: width/2, y: 370, width: 100, height: 30 },
                { id: 'output', label: 'Output', x: width/2, y: 430, width: 100, height: 40 }
            ];
            
            // Draw connections
            const connections = [
                { source: 'input', target: 'pos' },
                { source: 'pos', target: 'attention' },
                { source: 'attention', target: 'norm1' },
                { source: 'norm1', target: 'ffn' },
                { source: 'ffn', target: 'norm2' },
                { source: 'norm2', target: 'output' },
                // Skip connections
                { source: 'pos', target: 'norm1', skip: true },
                { source: 'norm1', target: 'norm2', skip: true }
            ];
            
            // Draw the connections
            connections.forEach(conn => {
                const source = components.find(c => c.id === conn.source);
                const target = components.find(c => c.id === conn.target);
                
                if (conn.skip) {
                    // Skip connection
                    svg.append('path')
                        .attr('d', `M${source.x + source.width/2} ${source.y + source.height/2} 
                                  C${source.x + source.width/2 + 60} ${source.y + source.height/2},
                                   ${target.x + target.width/2 + 60} ${target.y - target.height/2},
                                   ${target.x + target.width/2} ${target.y - target.height/2}`)
                        .attr('stroke', '#5D5CDE')
                        .attr('stroke-width', 2)
                        .attr('stroke-dasharray', '5,5')
                        .attr('fill', 'none');
                } else {
                    // Regular connection
                    svg.append('line')
                        .attr('x1', source.x)
                        .attr('y1', source.y + source.height)
                        .attr('x2', target.x)
                        .attr('y2', target.y)
                        .attr('stroke', '#5D5CDE')
                        .attr('stroke-width', 2);
                }
            });
            
            // Draw the components
            const boxes = svg.selectAll('g')
                .data(components)
                .enter()
                .append('g');
            
            boxes.append('rect')
                .attr('x', d => d.x - d.width/2)
                .attr('y', d => d.y - d.height/2)
                .attr('width', d => d.width)
                .attr('height', d => d.height)
                .attr('rx', 5)
                .attr('fill', '#5D5CDE')
                .attr('fill-opacity', 0.2)
                .attr('stroke', '#5D5CDE')
                .attr('stroke-width', 1);
            
            boxes.append('text')
                .attr('x', d => d.x)
                .attr('y', d => d.y + 5)
                .attr('text-anchor', 'middle')
                .attr('fill', 'currentColor')
                .text(d => d.label);
        }

        // Attention visualization
        function visualizeAttention() {
            const text = document.getElementById('input-text').value;
            if (!text.trim()) return;
            
            const tokens = text.split(/\s+/);
            const container = d3.select('#attention-visualization');
            container.html(''); // Clear previous visualization
            
            // Create a simple attention matrix (simulated values)
            const attentionMatrix = [];
            for (let i = 0; i < tokens.length; i++) {
                const row = [];
                for (let j = 0; j < tokens.length; j++) {
                    // Simulate attention scores with bias toward nearby tokens
                    const distance = Math.abs(i - j);
                    const attention = Math.exp(-distance / 3) / tokens.length;
                    row.push(attention);
                }
                attentionMatrix.push(row);
            }
            
            // Normalize attention scores for visualization
            for (let i = 0; i < attentionMatrix.length; i++) {
                const sum = attentionMatrix[i].reduce((a, b) => a + b, 0);
                for (let j = 0; j < attentionMatrix[i].length; j++) {
                    attentionMatrix[i][j] /= sum;
                }
            }
            
            // Create SVG container
            const width = container.node().clientWidth;
            const height = container.node().clientHeight;
            
            const svg = container.append('svg')
                .attr('width', width)
                .attr('height', height);
            
            const cellSize = Math.min(
                (width - 100) / tokens.length,
                (height - 80) / tokens.length
            );
            
            // Draw token labels across top
            svg.selectAll('.col-label')
                .data(tokens)
                .enter()
                .append('text')
                .attr('class', 'col-label')
                .attr('x', (d, i) => 80 + i * cellSize + cellSize / 2)
                .attr('y', 30)
                .attr('text-anchor', 'middle')
                .attr('font-size', '12px')
                .attr('fill', 'currentColor')
                .text(d => d);
            
            // Draw token labels down side
            svg.selectAll('.row-label')
                .data(tokens)
                .enter()
                .append('text')
                .attr('class', 'row-label')
                .attr('x', 70)
                .attr('y', (d, i) => 50 + i * cellSize + cellSize / 2)
                .attr('text-anchor', 'end')
                .attr('font-size', '12px')
                .attr('fill', 'currentColor')
                .text(d => d);
            
            // Draw attention heatmap
            for (let i = 0; i < tokens.length; i++) {
                for (let j = 0; j < tokens.length; j++) {
                    const attention = attentionMatrix[i][j];
                    
                    svg.append('rect')
                        .attr('x', 80 + j * cellSize)
                        .attr('y', 50 + i * cellSize)
                        .attr('width', cellSize)
                        .attr('height', cellSize)
                        .attr('fill', `rgba(93, 92, 222, ${attention * 5})`)
                        .attr('stroke', '#ccc')
                        .attr('stroke-width', 0.5);
                    
                    // Add attention value text for cells with significant attention
                    if (attention > 0.1) {
                        svg.append('text')
                            .attr('x', 80 + j * cellSize + cellSize / 2)
                            .attr('y', 50 + i * cellSize + cellSize / 2 + 4)
                            .attr('text-anchor', 'middle')
                            .attr('font-size', '10px')
                            .attr('fill', attention > 0.3 ? 'white' : 'black')
                            .text(attention.toFixed(2));
                    }
                }
            }
        }
        
        // Function to simulate token embeddings
        function simulateEmbedding(token, dimension = 5) {
            // Simple hash function to generate deterministic "embeddings"
            let hash = 0;
            for (let i = 0; i < token.length; i++) {
                hash = ((hash << 5) - hash) + token.charCodeAt(i);
                hash |= 0;
            }
            
            // Generate embedding vector components based on the hash
            const embedding = [];
            for (let i = 0; i < dimension; i++) {
                embedding.push(((hash + i) % 100) / 100 * 2 - 1);
            }
            
            return embedding;
        }
        
        // Function to simulate positional encoding
        function simulatePositionalEncoding(position, dimension = 5) {
            const encoding = [];
            for (let i = 0; i < dimension; i++) {
                const factor = i % 2 === 0 ? 
                    Math.sin(position / Math.pow(10000, i / dimension)) : 
                    Math.cos(position / Math.pow(10000, (i - 1) / dimension));
                encoding.push(factor);
            }
            return encoding;
        }
        
        // Function to simulate attention between tokens
        function simulateAttention(embeddings) {
            const len = embeddings.length;
            const attentionScores = [];
            
            for (let i = 0; i < len; i++) {
                const row = [];
                for (let j = 0; j < len; j++) {
                    // Simple dot product attention
                    let score = 0;
                    for (let k = 0; k < embeddings[i].length; k++) {
                        score += embeddings[i][k] * embeddings[j][k];
                    }
                    row.push(score);
                }
                attentionScores.push(row);
            }
            
            // Normalize scores with softmax
            for (let i = 0; i < len; i++) {
                const max = Math.max(...attentionScores[i]);
                const exp_scores = attentionScores[i].map(x => Math.exp(x - max));
                const sum = exp_scores.reduce((a, b) => a + b, 0);
                attentionScores[i] = exp_scores.map(x => x / sum);
            }
            
            return attentionScores;
        }
        
        // Function to simulate feed-forward network
        function simulateFFN(input) {
            // Simple 2-layer FFN simulation
            return input.map(x => 0.5 + 0.5 * Math.tanh(x));
        }
        
        // Process input through transformer demo
        function processInput() {
            const input = document.getElementById('demo-input').value;
            if (!input.trim()) return;
            
            // 1. Tokenization
            const tokens = input.split(/\s+/);
            document.getElementById('tokenization-output').innerHTML = 
                `<div class="flex flex-wrap gap-2">
                    ${tokens.map((t, i) => `<div class="px-2 py-1 bg-primary bg-opacity-20 rounded">${t} (token_id: ${i+1})</div>`).join('')}
                </div>`;
            
            // 2. Embedding + Positional Encoding
            const embeddings = tokens.map((token, idx) => {
                const embed = simulateEmbedding(token);
                const posEnc = simulatePositionalEncoding(idx);
                // Combine embedding and positional encoding
                return embed.map((val, i) => val + posEnc[i]);
            });
            
            document.getElementById('embedding-output').innerHTML = 
                `<div class="overflow-x-auto">
                    <table class="table-auto">
                        <thead>
                            <tr>
                                <th class="px-2 py-1">Token</th>
                                <th class="px-2 py-1">Embedding Vector (sample)</th>
                            </tr>
                        </thead>
                        <tbody>
                            ${tokens.map((token, idx) => `
                                <tr>
                                    <td class="px-2 py-1 border">${token}</td>
                                    <td class="px-2 py-1 border">[${embeddings[idx].map(v => v.toFixed(2)).join(', ')}]</td>
                                </tr>
                            `).join('')}
                        </tbody>
                    </table>
                </div>`;
            
            // 3. Self-Attention
            const attentionScores = simulateAttention(embeddings);
            const attentionHTML = `
                <div>
                    <h4 class="text-sm font-medium mb-2">Attention Scores</h4>
                    <div class="overflow-x-auto">
                        <table class="table-auto">
                            <thead>
                                <tr>
                                    <th class="px-2 py-1">From ↓ / To →</th>
                                    ${tokens.map(t => `<th class="px-2 py-1">${t}</th>`).join('')}
                                </tr>
                            </thead>
                            <tbody>
                                ${tokens.map((token, i) => `
                                    <tr>
                                        <td class="px-2 py-1 font-medium">${token}</td>
                                        ${attentionScores[i].map(score => `
                                            <td class="px-2 py-1 border" style="background-color: rgba(93, 92, 222, ${score * 0.8})">
                                                ${score.toFixed(2)}
                                            </td>
                                        `).join('')}
                                    </tr>
                                `).join('')}
                            </tbody>
                        </table>
                    </div>
                </div>
            `;
            document.getElementById('attention-output').innerHTML = attentionHTML;
            
            // 4. Feed-Forward Network
            const ffnOutputs = embeddings.map(embedding => simulateFFN(embedding));
            document.getElementById('ffn-output').innerHTML = 
                `<div class="overflow-x-auto">
                    <table class="table-auto">
                        <thead>
                            <tr>
                                <th class="px-2 py-1">Token</th>
                                <th class="px-2 py-1">FFN Output (sample)</th>
                            </tr>
                        </thead>
                        <tbody>
                            ${tokens.map((token, idx) => `
                                <tr>
                                    <td class="px-2 py-1 border">${token}</td>
                                    <td class="px-2 py-1 border">[${ffnOutputs[idx].map(v => v.toFixed(2)).join(', ')}]</td>
                                </tr>
                            `).join('')}
                        </tbody>
                    </table>
                </div>`;
            
            // 5. Final Output - project back to vocabulary logits
            const vocabulary = ["hello", "world", "the", "transformer", "model", "works", "input", "output", "attention", "neural"];
            const finalOutputs = tokens.map((token, idx) => {
                // Simulate logits for each token in vocabulary
                const logits = vocabulary.map((vocabToken, i) => {
                    // Higher logit for matching tokens and semantically similar ones
                    if (vocabToken === token.toLowerCase()) {
                        return 10.0;
                    } else {
                        // Simple similarity metric
                        return ffnOutputs[idx].reduce((sum, val, i) => 
                            sum + val * simulateEmbedding(vocabToken)[i % simulateEmbedding(vocabToken).length], 0);
                    }
                });
                
                // Apply softmax to get probabilities
                const max = Math.max(...logits);
                const exp_scores = logits.map(x => Math.exp(x - max));
                const sum = exp_scores.reduce((a, b) => a + b, 0);
                const probs = exp_scores.map(x => x / sum);
                
                return { token, logits: probs.map((p, i) => ({ token: vocabulary[i], prob: p }))};
            });
            
            document.getElementById('final-output').innerHTML = 
                `<div>
                    ${finalOutputs.map(output => `
                        <div class="mb-2">
                            <div class="font-medium">Token: ${output.token}</div>
                            <div class="overflow-x-auto mt-1">
                                <div class="flex flex-wrap gap-1">
                                    ${output.logits
                                        .sort((a, b) => b.prob - a.prob)
                                        .slice(0, 5)
                                        .map(logit => `
                                            <div class="px-2 py-1 bg-primary bg-opacity-${Math.floor(logit.prob * 100)} rounded">
                                                ${logit.token}: ${(logit.prob * 100).toFixed(1)}%
                                            </div>
                                        `).join('')}
                                </div>
                            </div>
                        </div>
                    `).join('')}
                </div>`;
        }
        
        // Initialize transformers diagram
        document.addEventListener('DOMContentLoaded', function() {
            drawTransformerDiagram();
            
            // Attach event handlers
            document.getElementById('visualize-btn').addEventListener('click', visualizeAttention);
            document.getElementById('process-btn').addEventListener('click', processInput);
            
            // Process initial data
            processInput();
        });
    </script>
</body>
</html>
